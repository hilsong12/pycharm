# -*- coding: utf-8 -*-
"""AI_exam04_heart_disease_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kqvaxSmG71a7TFSICc4m-fLQSU3R_0ce
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Dropout

column_names = ['age','sex','cp', 'treshbps', 'chol', 'fbs', ' restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal', 'HeartDisease']
raw_data = pd.read_excel('./heart-disease.xlsx', names = column_names)
print(raw_data.head())

print(raw_data.head())  #데이터 프레임이라는 객체
raw_data.tail()

raw_data.info()

raw_data.describe()   #판다스는 데이터를 다루기에 좋다. 테이블 형태의 자료형  object 칼럼은 인트 플롯이 아닌 계산할수 없는 객체

#물음표 제거
clean_data = raw_data.replace('?',np.nan)  #np.nan은 not a number 숫자가 아니다.
clean_data.info()                           #데이터 타입은 플로트 숫자인데 숫자가 아니다.? 계산을 할수 있는데 계산 결과가 nan 무한대 같은 느낌

"""nan 값을 어떻게 해야 할까? 결측치 처리? 결측값을 자료의 특성에 따라 평균,0, 최소, 최대
영향을 줄이기 위해서.  
1. 채우던가  (평균 | 최대 | 최소 | 0 등) -데이터 전처리 (데이터 전처리)
2. raw를 버리던가
3. column을 버리던가
"""

#데이터가 많으지 않으니 nan값들을 버리겠다.
clean_data = clean_data.dropna()
clean_data.info()

keep = column_names.pop()
print(keep)
print(column_names)

training_data = clean_data[column_names]
target_data = clean_data[[keep]]
print(training_data.head())
print(target_data.head())

print(target_data.sum())   #이진분류기를 할때 데이터를 반반씩 줘야한다. 한쪽에 치우치면 한쪽으로 쏠리게 되어있다.

print(target_data.mean())

from sklearn.preprocessing import StandardScaler  #싸이킷 런 -통계관련된 클래스를 가지고 있다. #평균 0 표준편차 1인걸로 만듬
# from sklearn.preprocessing import mimmaxScaler
acaler = StandardScaler()
scaled_data = acaler.fit_transform(training_data)
scaled_data = pd.DataFrame(scaled_data, columns = training_data.columns)
scaled_data.head()

"""민맥스 스탠다드스케일러 노말"""

scaled_data.describe().T  #T를 붙이면 가로 세로가 바뀜ㅍ

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(scaled_data,target_data,test_size=0.2)
print(X_train.shape)   #행236개 열 13개
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""검증데이터를 나누는건
데이터 시각화 사이킷(통계)
"""

y_test.mean()

model = Sequential()
model.add(Dense(256, input_dim=13, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation= 'sigmoid'))
model.summary()

model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics = ['binary_accuracy'])
fit_hist= model.fit(X_train, y_train, batch_size= 32, epochs= 50, validation_split= 0.2, verbose=1)

"""verbose는 출력형태를 알려준다.
batch_size는 문제집 다푸는게 아니라 몇문제 풀고 가중치 바꾸고를 반복하는거 메모리 적게들지만 시간이 오래걸림
validation_split 매 에폭마다 20퍼센트를 떼네어서 트레이닝 하지 않고 검증만함  
그 다음 에폭에서는 트레인이 됨.  (학습이 되는 데이터)
evaluate는 y_test를 하는 것. 이때는 학습하지 않는다.
optimizer는 경사하강 알고리즘 . 지역 최저점
아담이 런닝레이트를 조절한다. 멀면 런닝레이트가 크고 오차가 적으지면 런닝레이트를 낮춘다.
confuse metrics 혼돈 메트릭스
"""

score= model.evaluate(X_test, y_test, verbose=1)
print('loss', score[0])
print('accuracy', score[1])

plt.plot(fit_hist.history['binary_accuracy'])
plt.plot(fit_hist.history['val_binary_accuracy'])
plt.show()

"""과적합해서 답을 외웠다. plot을 그려보고 과적합이 일어나기전에 학습을 멈춘다."""

from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score

# 모델이 테스트 데이터(X_test)에 대해 예측한 확률값(0~1)을 반환
pred = model.predict(X_test)

# 확률을 이진 분류 값(True/False)으로 변환
# pred > 0.01 이면 True(=1), 아니면 False(=0)
# 일반적으로 0.5를 threshold로 많이 쓰지만, 데이터에 따라 임계값 조절 가능
pred = pred > 0.5

# 혼동 행렬 출력
# [[TN, FP],
#  [FN, TP]]
print(confusion_matrix(y_test, pred))

# F1 점수 출력
# F1 = 2 * (precision * recall) / (precision + recall)
# 정밀도와 재현율의 조화평균
print(f1_score(y_test, pred))

# 재현율(Recall) 출력
# 실제 양성 중에서 모델이 양성이라고 맞춘 비율
print(recall_score(y_test, pred))

# 정밀도(Precision) 출력
# 모델이 양성이라고 예측한 것 중에서 실제로 양성인 비율
print(precision_score(y_test, pred))

"""시그모이드라 0아니면 1이라 pred = pred> 0.5 로 했는데 좀더 보수적으로 하면
0.1로 하면.... 일단... 심장병 걸린사람은 다 잡을 수 있다.
"""

