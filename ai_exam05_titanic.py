# -*- coding: utf-8 -*-
"""AI_exam05_titanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v2Rz_tDeVK2WNm44impaiqXJqXnGyTs6
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns   #시각화 해주는 툴
import pandas as pd

raw_data = sns.load_dataset('titanic')
print(raw_data.head())

raw_data.info()

raw_data.isnull().sum()  #자료의 값이 null 이면 1이 나옴. 그걸 다 더함

"""deck같은 경우는 비어있는데이터가 많다.
nan은 실내에 있는 경우다 . 그래서 nan 값을 실내로 만들거드.
근데 지금은 실습을 위해 버리겠다.
"""

clean_data= raw_data.dropna(axis=1,thresh = 500)    #column은 1  raw는 0
print(clean_data.columns)   #deck 칼럼이 없다.

clean_data['age'].fillna(clean_data['age'].mean(),inplace=True)   #나이값 평균으로 채워 넣음 #inplace가 true 면 원본을 수정 false는 새로운 걸 만들어내서 리턴으로 받아줘야 함.
print(clean_data.isnull().sum())

clean_data.age[:20]

clean_data.drop(['embark_town', 'alive','class'],axis=1, inplace=True)   #axis = 1이면 칼럼 버리기
print(clean_data.columns)

clean_data['embarked'].fillna(method= 'ffill',inplace=True)  #ffill은 이전값으로 채우는 것.  bfill은 뒤에값으로 채우는 것.
print(clean_data.isnull().sum())

clean_data.info()

clean_data['sex'].replace({'male' :0, 'female':1},inplace= True)
print(clean_data['sex'].unique())  #unique는 한번만 보여주는 것.

print(clean_data.embarked.value_counts())

from re import L
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
clean_data['embarked'] = encoder.fit_transform(clean_data['embarked'])  #알파벳순으로 encoding을 해줌
print(clean_data['embarked'].value_counts())

print(clean_data.who.value_counts())

clean_data['who'] = encoder.fit_transform(clean_data['who'])
print(clean_data['who'].value_counts())

clean_data.info()   #bool 타입을 처리해줘야 한다.

clean_data['adult_male'] = clean_data['adult_male'].astype('int64')
clean_data['alone'] = clean_data['alone'].astype('int64')      #적절한 타입으로 바뀔수 있는 자료에는 바꿀수 있다.
clean_data.info()

"""데이터 처리를 해주어야 한다. 타겟뽑고 스케일링"""

target= clean_data['survived']
print(type(target))
print(target)

"""판다스는 데이터 프레임과 시리즈를 제공한다. 시리즈는 컬럼이 하나짜리 데이터 프레임은 칼럼이 여러개"""

target= clean_data[['survived']]
print(type(target))
print(target)

clean_data.columns

training_data= clean_data.drop(['survived'],axis=1)
training_data

"""다른 애들은 숫자가 안커서 age랑 fare만 전처리를 하겠다."""

value_data = training_data[['age', 'fare']]
print(value_data.head())

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(value_data)
print(type(scaled_data))
value_data =pd.DataFrame(scaled_data, columns=value_data.columns)
print(value_data.head())
print(type(value_data))

"""여자에서 여자빼면 남자가 되나? 그냥 라벨만 인덱스로 붙여놓은건데...
카테고리일 뿐이고 카테고리는 서로 계산이 안된다. 명목 척도...
명목척도 서열척도 등간척도 비율척도..\
명목척도는 수치가 아니라 라벨일 뿐이다.\
서열척도는 일등 이등 삼등 , 일등 이등 삼등이 간격이 일정하지 않다. 서열척도도 계산이 안된다. 그냥 순서만 있는것 \
등간척도는 간격이 일정해서 계산이 된다. \
비율척도는 일반적인거 절대 0이 있어야 한다. 길이.  등간척도도 계산이 되고 비율척도도 계산이 된다. 등간척도는 절대 0도가 없다. 온도가 없는게 아니다.
온도처럼 기준을 정하고 등간으로 나누면 등간척도
길이는 제로가 있다. 그런경우는 비율척도
\
남자컬럼 여자컬럼으로 나누면 계산이된다.? 남자냐 아니냐 남자면 1아니면 0
남자칼럼 계산해서 여자값 보는게 아니다. \
명목척도의 경우에는 서로 떼어 낸다.
"""

training_data.drop(['age','fare'], axis=1, inplace=True)
training_data

onehot_data= pd.get_dummies(training_data,columns= training_data.columns)
onehot_data   #유니크한 자료가 3개면 3개를 분할해서 컬럼이 나눠져 있다. 이렇게 하는걸 더미화 한다고 한다.

"""명목척도는 다 원핫코드로 \
굳이 더미화를 안해도 된다. 통계기법을 적용해봐야 더 나아지지 않는다.
통계를 배우면 더 좋은 모델을 만들수 있을까? 다 공평하다.
개발자의 의도를 모델에게 넣으면 더 안좋아진다.....
"""

onehot_data= onehot_data.astype('int')
onehot_data.info()

train_data = pd.concat([onehot_data,value_data],axis=1)
train_data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(train_data,target,test_size=0.2)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

target.mean()

man= raw_data.loc[raw_data.sex =='male']  #특정 자료만 뽑아 볼수 있다.
man.survived.mean()

woman= raw_data.loc[raw_data.sex =='female']
woman.survived.mean()

child= raw_data.loc[raw_data.age<13]
child.survived.mean()

clean_data.mean()

sur= clean_data.loc[clean_data.survived ==1]
sur.mean()  #여자가 1 남자가 0 살아남은 사람들중에 여자가 68프로

from keras.models import Sequential
from keras.layers import Dense,Dropout

print(X_train.shape)

model = Sequential()
model.add(Dense(128,input_dim=X_train.shape[1],activation= 'relu'))
model.add(Dropout(0.2))
model.add(Dense(256, activation  = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(512, activation  = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation  = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(1,activation= 'sigmoid'))
model.compile(loss = 'binary_crossentropy',optimizer= 'adam',metrics= ['accuracy'])
model.summary()

fit_hist = model.fit(X_train,y_train, epochs= 30, batch_size= 32, validation_data=(X_test, y_test))

plt.plot(fit_hist.history['accuracy'])
plt.plot(fit_hist.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

score = model.evaluate(X_test,y_test)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

